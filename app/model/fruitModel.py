"""
SAHI ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ YOLO ì¶”ë¡ 
ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ë¥¼ ìë™ìœ¼ë¡œ ìŠ¬ë¼ì´ì‹±í•˜ì—¬ ê°ì²´ íƒì§€ ìˆ˜í–‰
"""
from sahi import AutoDetectionModel
from sahi.predict import get_sliced_prediction
from sahi.utils.cv import read_image
from sahi.utils.file import download_from_url
import cv2
import numpy as np
from pathlib import Path


def predict_with_sahi(
        image_path: str,
        model_path: str,
        model_type: str = "yolov8",  # yolov8 ë˜ëŠ” yolov5
        slice_height: int = 640,
        slice_width: int = 640,
        overlap_height_ratio: float = 0.3,
        overlap_width_ratio: float = 0.3,
        conf_threshold: float = 0.3,
        iou_threshold: float = 0.3,
        postprocess_match_threshold: float = 0.5,
        save_path: str = "sahi_result.jpg"
):
    """
    SAHIë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ì—ì„œ ê°ì²´ íƒì§€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Args:
        image_path: ì…ë ¥ ì´ë¯¸ì§€ ê²½ë¡œ
        model_path: YOLO ëª¨ë¸ ê²½ë¡œ (.pt íŒŒì¼)
        model_type: ëª¨ë¸ íƒ€ì… ('yolov8' ë˜ëŠ” 'yolov5')
        slice_height: ìŠ¬ë¼ì´ìŠ¤ ë†’ì´
        slice_width: ìŠ¬ë¼ì´ìŠ¤ ë„ˆë¹„
        overlap_height_ratio: ë†’ì´ ì˜¤ë²„ë© ë¹„ìœ¨ (0.0~1.0)
        overlap_width_ratio: ë„ˆë¹„ ì˜¤ë²„ë© ë¹„ìœ¨ (0.0~1.0)
        conf_threshold: Confidence threshold
        iou_threshold: IoU threshold for NMS
        postprocess_match_threshold: ìŠ¬ë¼ì´ìŠ¤ ê°„ ë§¤ì¹­ì„ ìœ„í•œ IoU threshold
        save_path: ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ
    """
    print("=" * 70)
    print("SAHIë¥¼ ì‚¬ìš©í•œ ê°ì²´ íƒì§€")
    print("=" * 70)

    # 1. ëª¨ë¸ ë¡œë“œ
    print(f"\nğŸ“¦ ëª¨ë¸ ë¡œë“œ: {model_path}")
    detection_model = AutoDetectionModel.from_pretrained(
        model_type=model_type,
        model_path=model_path,
        confidence_threshold=conf_threshold,
        device="cpu"  # 'cuda:0' for GPU
    )

    # 2. ì´ë¯¸ì§€ ë¡œë“œ
    print(f"ğŸ“· ì´ë¯¸ì§€ ë¡œë“œ: {image_path}")
    image = read_image(image_path)
    print(f"   í¬ê¸°: {image.shape}")

    # 3. ìŠ¬ë¼ì´ìŠ¤ ì˜ˆì¸¡ ìˆ˜í–‰
    print(f"\nğŸ” ìŠ¬ë¼ì´ìŠ¤ ì¶”ë¡  ì‹œì‘...")
    print(f"   - ìŠ¬ë¼ì´ìŠ¤ í¬ê¸°: {slice_width}x{slice_height}")
    print(f"   - ì˜¤ë²„ë© ë¹„ìœ¨: {overlap_width_ratio * 100:.0f}%")
    print(f"   - Confidence threshold: {conf_threshold}")
    print(f"   - IoU threshold: {iou_threshold}")

    result = get_sliced_prediction(
        image,
        detection_model,
        slice_height=slice_height,
        slice_width=slice_width,
        overlap_height_ratio=overlap_height_ratio,
        overlap_width_ratio=overlap_width_ratio,
        postprocess_type="NMS",  # 'NMS' ë˜ëŠ” 'GREEDYNMM'
        postprocess_match_metric="IOS",  # 'IOU' ë˜ëŠ” 'IOS'
        postprocess_match_threshold=postprocess_match_threshold,
        postprocess_class_agnostic=False  # Falseë¡œ ì„¤ì •í•˜ì—¬ í´ë˜ìŠ¤ë³„ NMS
    )

    # 4. ê²°ê³¼ ë¶„ì„
    print(f"\nâœ… ì¶”ë¡  ì™„ë£Œ")
    print(f"ğŸ¯ ê²€ì¶œ ê°œìˆ˜: {len(result.object_prediction_list)}")

    # í´ë˜ìŠ¤ë³„ í†µê³„
    class_counts = {}
    confidences = []

    for i, pred in enumerate(result.object_prediction_list):
        class_id = pred.category.id
        class_name = pred.category.name
        confidence = pred.score.value
        bbox = pred.bbox.to_voc_bbox()  # [x1, y1, x2, y2]

        # í†µê³„ ìˆ˜ì§‘
        if class_name not in class_counts:
            class_counts[class_name] = 0
        class_counts[class_name] += 1
        confidences.append(confidence)

        print(f"   [{i + 1}] í´ë˜ìŠ¤: {class_name} (ID: {class_id}), "
              f"ì‹ ë¢°ë„: {confidence:.3f}, bbox: {bbox}")

    # 5. í†µê³„ ì¶œë ¥
    if len(result.object_prediction_list) > 0:
        print(f"\nğŸ“Š ê²€ì¶œ í†µê³„:")
        for class_name, count in class_counts.items():
            print(f"   - {class_name}: {count}ê°œ")
        print(f"\nğŸ“ˆ Confidence ë¶„í¬:")
        print(f"   - í‰ê· : {np.mean(confidences):.3f}")
        print(f"   - ìµœëŒ€: {np.max(confidences):.3f}")
        print(f"   - ìµœì†Œ: {np.min(confidences):.3f}")

    # 6. ìŠ¬ë¼ì´ìŠ¤ ì˜ì—­ ì‹œê°í™”
    slice_vis_path = save_path.replace(".jpg", "_slices.jpg")
    visualize_slices(image_path, slice_height, slice_width,
                     overlap_height_ratio, overlap_width_ratio,
                     slice_vis_path)

    # 7. ê²°ê³¼ ì‹œê°í™” ë° ì €ì¥
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {save_path}")
    export_dir = str(Path(save_path).parent)
    result.export_visuals(export_dir=export_dir)

    # SAHIì˜ ê¸°ë³¸ ì‹œê°í™” íŒŒì¼ëª… -> ì›í•˜ëŠ” íŒŒì¼ëª…ìœ¼ë¡œ ë®ì–´ì“°ê¸°
    default_export_path = Path(export_dir) / "prediction_visual.png"
    target_path = Path(save_path)

    if default_export_path.exists():
        # WindowsëŠ” renameì´ ë®ì–´ì“°ê¸°ë¥¼ ëª» í•˜ë¯€ë¡œ ë¨¼ì € ì‚­ì œ
        if target_path.exists():
            target_path.unlink()
        default_export_path.replace(target_path)  # âœ… ë®ì–´ì“°ê¸° ì´ë™
        print(f"   â†’ {target_path}")

    # ì¶”ê°€ ì»¤ìŠ¤í…€ ì‹œê°í™” (ì„ íƒì‚¬í•­)
    vis_image = visualize_custom(image_path, result.object_prediction_list, save_path.replace(".jpg", "_custom.jpg"))

    print("=" * 70)

    return result.object_prediction_list


def visualize_slices(image_path: str, slice_height: int, slice_width: int,
                     overlap_height_ratio: float, overlap_width_ratio: float,
                     save_path: str):
    """
    ìŠ¬ë¼ì´ìŠ¤ ì˜ì—­ì„ ì‹œê°í™”
    """
    image = cv2.imread(image_path)
    if image is None:
        print(f"âš ï¸ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        return None

    height, width = image.shape[:2]

    # ìŠ¬ë¼ì´ìŠ¤ ê³„ì‚°
    stride_h = int(slice_height * (1 - overlap_height_ratio))
    stride_w = int(slice_width * (1 - overlap_width_ratio))

    # ë°˜íˆ¬ëª… ì˜¤ë²„ë ˆì´ ìƒì„±
    overlay = image.copy()

    slice_count = 0
    for y in range(0, height, stride_h):
        for x in range(0, width, stride_w):
            # ìŠ¬ë¼ì´ìŠ¤ ê²½ê³„ ê³„ì‚°
            x1 = x
            y1 = y
            x2 = min(x + slice_width, width)
            y2 = min(y + slice_height, height)

            slice_count += 1

            # êµì°¨í•˜ëŠ” ìƒ‰ìƒìœ¼ë¡œ ê²½ê³„ ê·¸ë¦¬ê¸°
            color = (0, 255, 255) if (slice_count % 2 == 0) else (255, 0, 255)
            cv2.rectangle(overlay, (x1, y1), (x2, y2), color, 3)

            # ìŠ¬ë¼ì´ìŠ¤ ë²ˆí˜¸ í‘œì‹œ
            font_scale = max(0.6, min(width, height) / 2000)
            thickness = max(1, int(2 * font_scale))
            text = f"#{slice_count}"
            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)

            # í…ìŠ¤íŠ¸ ë°°ê²½
            text_x = x1 + 10
            text_y = y1 + text_size[1] + 10
            cv2.rectangle(overlay,
                          (text_x - 5, text_y - text_size[1] - 5),
                          (text_x + text_size[0] + 5, text_y + 5),
                          (0, 0, 0), -1)

            # í…ìŠ¤íŠ¸
            cv2.putText(overlay, text, (text_x, text_y),
                        cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), thickness)

    # ë°˜íˆ¬ëª… ë¸”ë Œë”©
    alpha = 0.7
    result = cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0)

    cv2.imwrite(save_path, result)
    print(f"ğŸ’¾ ìŠ¬ë¼ì´ìŠ¤ ì‹œê°í™” ì €ì¥: {save_path} (ì´ {slice_count}ê°œ ìŠ¬ë¼ì´ìŠ¤)")

    return result


def visualize_custom(image_path: str, predictions: list, save_path: str):
    """
    ì»¤ìŠ¤í…€ ì‹œê°í™” (OK/NG ìƒ‰ìƒ êµ¬ë¶„)
    """
    image = cv2.imread(image_path)
    if image is None:
        print(f"âš ï¸ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        return None

    # í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ (0: OK=ì´ˆë¡, 1: NG=ë¹¨ê°•)
    colors = {
        0: (0, 255, 0),  # OK: ì´ˆë¡
        1: (0, 0, 255),  # NG: ë¹¨ê°•
        "default": (255, 0, 0)  # ê¸°ë³¸: íŒŒë‘
    }

    class_names = {0: "OK", 1: "NG"}

    # ì´ë¯¸ì§€ í¬ê¸°ì— ë”°ë¼ ë™ì  ì¡°ì •
    img_area = image.shape[0] * image.shape[1]
    scale_factor = np.sqrt(img_area / (640 * 640))
    thickness = max(1, int(2 * scale_factor))
    font_scale = max(0.5, 0.6 * scale_factor)

    for pred in predictions:
        class_id = pred.category.id
        class_name = pred.category.name if hasattr(pred.category, 'name') else class_names.get(class_id, str(class_id))
        confidence = pred.score.value
        bbox = pred.bbox.to_voc_bbox()  # [x1, y1, x2, y2]

        x1, y1, x2, y2 = map(int, bbox)
        color = colors.get(class_id, colors["default"])

        # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)

        # ë¼ë²¨ ê·¸ë¦¬ê¸°
        label = f"{class_name}: {confidence:.2f}"
        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)

        # ë°°ê²½ ë°•ìŠ¤
        cv2.rectangle(image,
                      (x1, y1 - label_size[1] - 10),
                      (x1 + label_size[0], y1),
                      color, -1)

        # í…ìŠ¤íŠ¸
        cv2.putText(image, label,
                    (x1, y1 - 5),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    font_scale,
                    (255, 255, 255),
                    thickness)

        # ì¤‘ì‹¬ì 
        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2
        cv2.circle(image, (cx, cy), max(3, int(5 * scale_factor)), color, -1)

    cv2.imwrite(save_path, image)
    print(f"ğŸ’¾ ì»¤ìŠ¤í…€ ì‹œê°í™” ì €ì¥: {save_path}")

    return image


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="SAHIë¥¼ ì‚¬ìš©í•œ YOLO ì¶”ë¡ ")
    parser.add_argument("--image", type=str,
                        default="/Users/temp/ë‚´ ë“œë¼ì´ë¸Œ(codejeteho123@gmail.com)/ComputerVision/sample_1920x1080.jpg",
                        help="í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ")
    parser.add_argument("--model", type=str,
                        default="runs/detect/strawberry_ok_ng/weights/best.pt",
                        help="YOLO ëª¨ë¸ ê²½ë¡œ (.pt íŒŒì¼)")
    parser.add_argument("--model-type", type=str, default="yolov8",
                        choices=["yolov8", "yolov5"],
                        help="ëª¨ë¸ íƒ€ì…")
    parser.add_argument("--slice-size", type=int, default=640,
                        help="ìŠ¬ë¼ì´ìŠ¤ í¬ê¸° (ì •ì‚¬ê°í˜•)")
    parser.add_argument("--overlap", type=float, default=0.3,
                        help="ì˜¤ë²„ë© ë¹„ìœ¨ (0.0~1.0)")
    parser.add_argument("--conf", type=float, default=0.85,
                        help="Confidence threshold")
    parser.add_argument("--iou", type=float, default=0.3,
                        help="IoU threshold")
    parser.add_argument("--output", type=str, default="sahi_result.jpg",
                        help="ì¶œë ¥ ì´ë¯¸ì§€ ê²½ë¡œ")

    args = parser.parse_args()

    try:
        predictions = predict_with_sahi(
            image_path=args.image,
            model_path=args.model,
            model_type=args.model_type,
            slice_height=args.slice_size,
            slice_width=args.slice_size,
            overlap_height_ratio=args.overlap,
            overlap_width_ratio=args.overlap,
            conf_threshold=args.conf,
            iou_threshold=args.iou,
            save_path=args.output
        )
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("\nğŸ’¡ SAHI ì„¤ì¹˜ í™•ì¸:")
        print("   pip install sahi")
        raise